import logging
import os
import random
import json
import time
from typing import Union, Tuple, Optional, Dict, Any
import re # Import regex

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv

# Importing LangChain components
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
# from langchain_google_genai import ChatGoogleGenerativeAI # Keep if needed
from langchain_core.tools import tool
from langchain_community.document_loaders import Docx2txtLoader
from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# ---------------- Logging Setup ----------------
logging.basicConfig(
    level=logging.INFO, # Set to INFO for production, DEBUG for detailed tracing
    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)
logging.getLogger("httpx").setLevel(logging.WARNING) # Quieten noisy libraries
logging.getLogger("langchain_community").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING) # Quieten requests library


# ---------------- Environment Setup ----------------
load_dotenv()

# Validate API keys
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.critical("OPENAI_API_KEY not found in environment variables.")
    raise EnvironmentError("OPENAI_API_KEY must be set.")

# ---------------- LLM Initialization ----------------
llm = ChatOpenAI(
    model="gpt-4o-mini", # Use gpt-4o if mini struggles with complex reasoning/scraping instructions
    temperature=0.1, # Slightly increased temp for potentially broader search ideas
    max_tokens=None,
    timeout=None,
    max_retries=3, # Increase retries slightly
)
logger.info(f"Initialized LLM: {llm.model_name}")

# ---------------- Tool Definitions ----------------
@tool
def jobsdb_search(keywords: str, location: str = "Hong Kong", page: int = 1) -> str:
    """
    Searches for job listings on JobsDB Hong Kong based on keywords and location.
    Use this tool to get an initial list of potentially relevant jobs from JobsDB.
    Specify the 'page' number for pagination (default is 1).
    Returns a list of job summaries including Job IDs.

    Args:
        keywords (str): Specific keywords for the job search (e.g., "software engineer python", "project manager fintech startup").
        location (str): The target location (default is "Hong Kong"). Specify if different.
        page (int): The page number of results to fetch (default is 1).
    """
    try:
        logger.info(f"Initiating JobsDB search. Keywords: '{keywords}', Location: '{location}', Page: {page}")

        # Basic keyword cleaning/encoding
        kw_param = requests.utils.quote(keywords)
        loc_param = requests.utils.quote(location)

        # Note: The JobsDB API structure might change. This is based on observed patterns.
        # The 'baseKeywords' seemed less critical than primary keywords in testing, simplifying.
        jobsdb_url = (
            f"https://hk.jobsdb.com/api/jobsearch/v5/search?siteKey=HK-Main&sourcesystem=houston"
            f"&page={page}&keywords={kw_param}&pageSize=20" # Reduced page size slightly
            f"&locale=en-HK&location={loc_param}"
            f"&include=seodata,jobDetailScore" # Simplified includes
        )
        logger.debug(f"Requesting JobsDB URL: {jobsdb_url}")

        # Use a session for potential connection reuse
        with requests.Session() as session:
            session.headers.update({ # Add basic headers
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',
                'Accept': 'application/json',
            })
            response = session.get(jobsdb_url, timeout=25) # Increased timeout
            response.raise_for_status() # Check for HTTP errors (4xx, 5xx)

        data = response.json()
        jobs_data = data.get("data", [])

        if not jobs_data:
            logger.warning(f"No JobsDB jobs found for keywords: '{keywords}', Location: '{location}', Page: {page}.")
            return f"No JobsDB jobs found for '{keywords}' in {location} on page {page}."

        formatted_jobs = f"Found {len(jobs_data)} JobsDB jobs (Page {page}) for '{keywords}' in {location}:\n"
        for job in jobs_data:
            formatted_jobs += "=" * 50 + "\n"
            formatted_jobs += format_job(job) + "\n" # Use helper function

        logger.info(f"Successfully retrieved {len(jobs_data)} job summaries from JobsDB.")
        # Return full results for the agent to evaluate
        return formatted_jobs

    except requests.exceptions.Timeout:
        logger.error(f"Timeout error searching JobsDB for '{keywords}'. URL: {jobsdb_url}")
        return f"Error: Timeout occurred while searching JobsDB for '{keywords}'."
    except requests.exceptions.RequestException as e:
        logger.error(f"HTTP Error searching JobsDB: {e}. URL: {jobsdb_url}")
        # Include status code if available
        status_code = e.response.status_code if e.response is not None else 'N/A'
        return f"Error: Could not connect to JobsDB or received an error (Status: {status_code}) for '{keywords}': {e}"
    except json.JSONDecodeError as e:
        logger.error(f"JSON Parsing Error for JobsDB response for '{keywords}': {e}. Response text: {response.text[:500]}") # Log start of bad response
        return f"Error: Could not parse the response from JobsDB for '{keywords}'."
    except Exception as e:
        logger.error(f"Unexpected error searching JobsDB for '{keywords}': {e}", exc_info=True)
        return f"Error: An unexpected error occurred during JobsDB search: {e}"


@tool
def linkedin_search(keywords: str, location: str = "Hong Kong", page: int = 1) -> str:
    """
    Searches for job listings on LinkedIn (Guest Access) based on keywords and location.
    Use this tool to find jobs on LinkedIn, especially if JobsDB results are insufficient
    or for roles commonly listed on LinkedIn (tech, international).
    Specify the 'page' number (default 1). Note: LinkedIn scraping is unreliable.
    Returns a list of job summaries including Job IDs.

    Args:
        keywords (str): Specific keywords for the job search (e.g., "data scientist", "marketing manager google").
        location (str): Location for the job search (e.g., "Hong Kong", "Singapore"). Default is "Hong Kong".
        page (int): The page number of results to fetch (starts at 1). LinkedIn shows 25 per page.
    """
    logger.info(f"Initiating LinkedIn search (Guest). Keywords: '{keywords}', Location: '{location}', Page: {page}")
    # Disclaimer about reliability
    reliability_warning = "[Note: LinkedIn scraping is unreliable and may fail or miss jobs.]\n"

    try:
        # Calculate start index for LinkedIn pagination (0-based)
        start_index = (page - 1) * 25
        if start_index < 0: start_index = 0 # Ensure non-negative start

        # URL encode parameters
        kw_param = requests.utils.quote(keywords)
        loc_param = requests.utils.quote(location)

        # Using the guest API endpoint for job listings
        list_url = (
            f"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search"
            f"?keywords={kw_param}&location={loc_param}"
            f"&start={start_index}"
            # Adding common params observed, might help mimic browser
            f"&f_TPR=r86400" # Past 24 hours - consider removing or making optional if too restrictive
            f"&trk=public_jobs_jobs-search-bar_search-submit"
            f"&position=1&pageNum=0" # These might relate to internal tracking
        )
        logger.debug(f"Requesting LinkedIn Guest API URL: {list_url}")

        # Use scrape_full_html which includes headers and delay
        response_text = scrape_full_html_original(list_url) # Returns HTML content or error string
        logger.info(f"LinkedIn Raw HTML received (first 1000 chars): {response_text[:1000] if response_text else 'None'}")
        if response_text and ("captcha" in response_text.lower() or "sign in" in response_text.lower() or "authenticate" in response_text.lower()):
            logger.warning("Potential CAPTCHA or login block detected on LinkedIn search page.")
        if response_text is None or response_text.startswith("Error:"):
            logger.error(f"Failed to scrape LinkedIn search results page: {response_text}")
            return reliability_warning + (response_text or "Error: Failed to get response from LinkedIn.")

        # --- Parsing Logic ---
        soup = BeautifulSoup(response_text, 'html.parser')
        job_cards = soup.find_all('li') # Job postings are often within <li> tags in this view

        jobs_found = []
        if not job_cards:
            # Sometimes the structure changes, look for a different container
            job_container = soup.find('ul', class_=re.compile(r'jobs-search__results-list'))
            if job_container:
                job_cards = job_container.find_all('li')

        logger.debug(f"Found {len(job_cards)} potential job elements in LinkedIn HTML.")

        for card in job_cards:
            job_id = None
            # Try finding job ID from the card's data-entity-urn or data-id
            data_urn = card.get('data-entity-urn')
            if data_urn and 'jobPosting:' in data_urn:
                job_id = data_urn.split(':')[-1]

            if not job_id:
                # Fallback: Look for job ID in links within the card
                 link_tag = card.find('a', class_=re.compile(r'job-card-list__title|base-card__full-link'))
                 if link_tag and link_tag.has_attr('href'):
                     match = re.search(r'/jobs/view/(\d+)', link_tag['href'])
                     if match:
                         job_id = match.group(1)

            if not job_id: # Skip if no ID could be reliably extracted
                 logger.debug("Skipping card, could not extract job ID.")
                 continue

            title_tag = card.find(['h3', 'span'], class_=re.compile(r'base-search-card__title|job-card-list__title'))
            company_tag = card.find(['h4', 'a'], class_=re.compile(r'base-search-card__subtitle|hidden-nested-link'))
            location_tag = card.find('span', class_=re.compile(r'job-search-card__location|job-card-container__metadata-item'))
            date_tag = card.find('time', class_=re.compile(r'job-search-card__listdate'))

            # Clean up text extraction
            title = title_tag.text.strip() if title_tag else "N/A"
            company = company_tag.text.strip() if company_tag else "N/A"
            location_text = location_tag.text.strip() if location_tag else "N/A"
            posted = date_tag.text.strip() if date_tag else (date_tag['datetime'] if date_tag and date_tag.has_attr('datetime') else "N/A")

            job_info = {
                "job_id": job_id,
                "title": title,
                "company": company,
                "location": location_text,
                "posted": posted,
                "platform": "linkedin" # Add platform marker
            }
            jobs_found.append(job_info)

        if not jobs_found:
            logger.warning(f"No parsable LinkedIn job listings found for keywords: '{keywords}', Location: '{location}', Page: {page}.")
            # Check if it's a captcha or block page
            if "captcha" in response_text.lower() or "authenticate" in response_text.lower():
                 logger.error("LinkedIn scraping likely blocked (Captcha/Auth detected).")
                 return reliability_warning + f"Error: LinkedIn search blocked for '{keywords}'. Access denied or captcha required."
            return reliability_warning + f"No LinkedIn jobs found (or couldn't parse results) for '{keywords}' in {location} on page {page}."

        # Format output
        formatted_jobs = reliability_warning
        formatted_jobs += f"Found {len(jobs_found)} LinkedIn jobs (Page {page}) for '{keywords}' in {location}:\n"
        for idx, job in enumerate(jobs_found, 1):
             formatted_jobs += (
                 f"{'=' * 50}\n"
                 f"Job Index: {idx}\n" # Use index for clarity
                 f"Platform : LinkedIn\n"
                 f"Job ID   : {job['job_id']}\n"
                 f"Title    : {job['title']}\n"
                 f"Company  : {job['company']}\n"
                 f"Location : {job['location']}\n"
                 f"Posted   : {job['posted']}\n"
             )

        logger.info(f"Successfully parsed {len(jobs_found)} job summaries from LinkedIn Guest Search.")
        return formatted_jobs

    except requests.exceptions.Timeout:
        logger.error(f"Timeout error during LinkedIn search for '{keywords}'.")
        return reliability_warning + f"Error: Timeout occurred while searching LinkedIn for '{keywords}'."
    except requests.exceptions.RequestException as e:
        logger.error(f"HTTP Error during LinkedIn search: {e}.")
        status_code = e.response.status_code if e.response is not None else 'N/A'
        return reliability_warning + f"Error: Could not connect to LinkedIn or received an error (Status: {status_code}) for '{keywords}': {e}"
    except Exception as e:
        logger.error(f"Unexpected error searching LinkedIn for '{keywords}': {e}", exc_info=True)
        return reliability_warning + f"Error: An unexpected error occurred during LinkedIn search: {e}"


@tool
def extract_job_details(job_id: str, platform: str) -> str:
    """
    Extracts the full job title and a concise summary of requirements/responsibilities
    for a specific job ID obtained from 'jobsdb_search' or 'linkedin_search'.
    It scrapes the job's detail page and uses an LLM for summarization.
    Use this *after* identifying a promising job ID from search results.
    Note: Scraping detail pages, especially LinkedIn, might fail.

    Args:
        job_id (str): The specific ID of the job (e.g., "JHK100003009123456" for JobsDB, or a numerical ID like "3881234567" for LinkedIn).
        platform (str): The platform the job ID belongs to. Must be 'jobsdb' or 'linkedin'.
    """
    platform = platform.lower()
    logger.info(f"Extracting details for {platform.upper()} job ID: {job_id}")

    if platform == 'jobsdb':
        url = f"https://hk.jobsdb.com/job/{job_id}"
    elif platform == 'linkedin':
        # Construct the standard LinkedIn job view URL
        url = f"https://www.linkedin.com/jobs/view/{job_id}" # Use www, might be more stable for guest view
    else:
        logger.error(f"Invalid platform '{platform}' specified for job ID {job_id}.")
        return "Error: Invalid platform specified. Must be 'jobsdb' or 'linkedin'."

    logger.debug(f"Attempting to scrape job details from URL: {url}")
    text = scrape_all_text_original(url) # Use the robust text scraper

    if text is None or text.startswith("Error:"): # Check if scraping failed or returned an error string
        error_message = f"Error: Unable to scrape content from {url} for {platform.upper()} job ID {job_id}. Cannot extract details. Reason: {text or 'No content returned'}"
        logger.error(error_message)
        # Return the scraping error directly to the agent
        return error_message

    if not text.strip():
         error_message = f"Error: Scraped content from {url} for {platform.upper()} job ID {job_id} was empty. Cannot extract details."
         logger.error(error_message)
         return error_message

    # Limit text size sent to LLM (important for cost and context window)
    max_chars = 18000 # Increased slightly for potentially longer descriptions
    if len(text) > max_chars:
        logger.warning(f"{platform.upper()} job description text long ({len(text)} chars), truncating to {max_chars} for LLM analysis.")
        text_to_send = text[:max_chars] + "\n... [Content Truncated]"
    else:
        text_to_send = text

    # Consistent prompt for extraction
    prompt_extract = f"""
    Given the following text scraped from a job posting page ({platform.upper()}, Job ID: {job_id}) at URL {url}:
    --- START TEXT ---
    {text_to_send}
    --- END TEXT ---

    Analyze the text and extract the following information:
    1.  "title": The full, specific job title mentioned in the posting (e.g., "Senior Software Engineer (Backend, Java)", "Marketing Director - APAC").
    2.  "summary": A concise summary (3-5 sentences) covering the core responsibilities and key requirements (skills, experience level, qualifications). Focus on details relevant for matching with a CV. Highlight any salary information if explicitly stated.

    Return the result ONLY in the following JSON format:
    ```json
    {{
      "title": "...",
      "summary": "..."
    }}
    ```
    If the title or summary cannot be reliably extracted (e.g., page is an error, irrelevant content), use the string "Not Found" as the value. Do not add explanations outside the JSON structure. Check if the text seems like a valid job description before extracting. If it looks like an error page or login prompt, set both fields to "Not Found".
    """
    try:
        logger.debug(f"Invoking LLM for {platform.upper()} job detail extraction (ID: {job_id}).")
        messages = [HumanMessage(content=prompt_extract)]
        response = llm.invoke(messages)
        response_content = response.content.strip()

        logger.debug(f"Raw LLM response for extraction (Job ID: {job_id}):\n{response_content}")

        # Attempt to parse the JSON response (more robustly)
        try:
            # Find JSON block within potential markdown fences or other text
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_content, re.DOTALL)
            if json_match:
                json_string = json_match.group(1)
            else:
                 # Fallback: assume the whole response is JSON if no fences found
                 json_string = response_content
                 # Basic check if it looks like JSON
                 if not json_string.startswith('{') or not json_string.endswith('}'):
                     raise json.JSONDecodeError("Response does not appear to be JSON.", json_string, 0)


            # Clean potential escape sequences before parsing
            json_string = json_string.replace('\\n', '\n').replace('\\"', '"')

            details_json = json.loads(json_string)
            title = details_json.get("title", "Extraction Error: Title Key Missing")
            summary = details_json.get("summary", "Extraction Error: Summary Key Missing")

            if title == "Not Found" and summary == "Not Found":
                logger.warning(f"LLM indicated title and summary not found for {platform.upper()} job {job_id} (URL: {url}). Might be error page or invalid content.")
                return f"Platform: {platform.upper()}\nJob ID: {job_id}\nStatus: Could not extract title or summary from the job page content (URL: {url}). Content might be invalid or inaccessible."
            elif title == "Not Found":
                logger.warning(f"LLM indicated title not found for {platform.upper()} job {job_id}.")
                title = "[Title Not Found in Content]" # Use clearer status
            elif summary == "Not Found":
                logger.warning(f"LLM indicated summary not found for {platform.upper()} job {job_id}.")
                summary = "[Summary Not Found in Content]" # Use clearer status


            result_str = (f"Platform: {platform.upper()}\n"
                          f"Job ID  : {job_id}\n"
                          f"Title   : {title}\n"
                          f"Summary : {summary}")
            logger.info(f"Successfully extracted details for {platform.upper()} job ID {job_id}.")
            return result_str

        except json.JSONDecodeError as json_e:
            error_message = f"Error: Failed to parse JSON response from LLM for {platform.upper()} job {job_id}. Raw response: '{response_content}'. Error: {json_e}"
            logger.error(error_message)
            # Fallback: Return raw response if parsing fails, indicating an issue
            return f"Platform: {platform.upper()}\nJob ID: {job_id}\nStatus: Error processing LLM response - JSON parsing failed. Raw output: {response_content}"
        except Exception as parse_e:
            error_message = f"Error: Unexpected error parsing LLM response for {platform.upper()} job {job_id}. Error: {parse_e}"
            logger.error(error_message, exc_info=True)
            return f"Platform: {platform.upper()}\nJob ID: {job_id}\nStatus: Error processing LLM response. Details: {parse_e}"


    except Exception as e:
        # Catch errors during the LLM call itself
        error_message = f"Error: LLM invocation failed during job detail extraction for {platform.upper()} job {job_id}: {e}"
        logger.error(error_message, exc_info=True)
        return error_message

# ---------------- Helper Functions ----------------
def scrape_full_html_original(url: str) -> Optional[str]:
    """
    Scrape and return the FULL RAW HTML from the given URL.
    (Uses randomized user-agent, delay, specific headers - Adapt from your original code)
    Returns error message string on failure.
    """
    user_agents = [ # Keep your original list if preferred
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        # ... other agents
    ]
    headers = { # Use headers appropriate for fetching raw HTML (maybe more browser-like)
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'DNT': '1',
        'Sec-GPC': '1',
        # Add other headers from your original scrape_full_html if they were different
    }
    try:
        delay = random.uniform(1.5, 4.0) # Adjust delay if needed
        logger.debug(f"(Original HTML Scraper) Scraping URL: {url} (Delay: {delay:.2f}s)")
        time.sleep(delay)
        response = requests.get(url, headers=headers, timeout=25) # Adjust timeout
        response.raise_for_status()

        content_type = response.headers.get('Content-Type', '').lower()
        if 'html' not in content_type:
             logger.warning(f"(Original HTML Scraper) Non-HTML content type '{content_type}' received from {url}")
             return f"Error: Received non-HTML content ({content_type}) from {url}"

        logger.debug(f"(Original HTML Scraper) Successfully scraped HTML from {url} (Length: {len(response.text)}).")
        return response.text # Return raw HTML

    except requests.exceptions.Timeout:
        logger.error(f"(Original HTML Scraper) Timeout error fetching URL {url}")
        return f"Error: Timeout occurred while trying to fetch {url}"
    # ... add other specific except blocks from your original scrape_full_html ...
    except requests.exceptions.RequestException as e:
        logger.error(f"(Original HTML Scraper) Request error fetching URL {url}: {e}")
        status_code = e.response.status_code if e.response is not None else 'N/A'
        return f"Error: Failed to fetch {url} (Status: {status_code}): {e}"
    except Exception as e:
        logger.error(f"(Original HTML Scraper) Unexpected error scraping URL {url}: {e}", exc_info=True)
        return f"Error: An unexpected error occurred during scraping: {e}"


def scrape_all_text_original(url: str) -> Optional[str]:
    """
    Scrape and return cleaned TEXT content from the given URL.
    (Uses randomized user-agent, delay, specific headers, specific cleaning logic - Adapt from your original code)
    Returns error message string on failure.
    """
    user_agents = [ # Keep your original list if preferred
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
         # ... other agents
    ]
    headers = { # Use headers appropriate for getting content to extract text from
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        # Add other headers from your original scrape_all_text if they were different
    }
    try:
        delay = random.uniform(1.5, 4.0) # Adjust delay if needed
        logger.debug(f"(Original Text Scraper) Scraping URL: {url} (Delay: {delay:.2f}s)")
        time.sleep(delay)
        response = requests.get(url, headers=headers, timeout=25) # Adjust timeout
        response.raise_for_status()

        content_type = response.headers.get('Content-Type', '').lower()
        if 'html' not in content_type:
             logger.warning(f"(Original Text Scraper) Non-HTML content type '{content_type}' received from {url}")
             return f"Error: Received non-HTML content ({content_type}) from {url}"

        soup = BeautifulSoup(response.content, 'html.parser')

        # --- Your Original Cleaning Logic Here ---
        # Example: replicate the cleaning from the *first* version you posted
        for element in soup(["script", "style", "header", "footer", "nav", "aside"]): # Use the exact tags you had
             element.decompose()
        text = soup.get_text(separator='\n', strip=True)
        # Add any other specific cleaning steps you had in the original scrape_all_text
        # -----------------------------------------

        if not text.strip():
             logger.warning(f"(Original Text Scraper) Extracted text from {url} is empty after cleaning.")
             # Add checks for block pages if needed
             return f"Error: Extracted text from {url} is empty."

        logger.debug(f"(Original Text Scraper) Successfully scraped and extracted text from {url} (Length: {len(text)}).")
        return text

    except requests.exceptions.Timeout:
        logger.error(f"(Original Text Scraper) Timeout error fetching URL {url}")
        return f"Error: Timeout occurred while trying to fetch {url}"
    # ... add other specific except blocks from your original scrape_all_text ...
    except requests.exceptions.RequestException as e:
        logger.error(f"(Original Text Scraper) Request error fetching URL {url}: {e}")
        status_code = e.response.status_code if e.response is not None else 'N/A'
        return f"Error: Failed to fetch {url} (Status: {status_code}): {e}"
    except Exception as e:
        logger.error(f"(Original Text Scraper) Unexpected error scraping URL {url}: {e}", exc_info=True)
        return f"Error: An unexpected error occurred during scraping: {e}"


def format_job(job: dict) -> str:
    """Format and return job details from a JobsDB job dictionary."""
    # Adapt based on actual JobsDB API response structure observed
    title = job.get("title", "N/A")
    company_data = job.get("companyMeta", {}) # Look in companyMeta
    company = company_data.get("name", job.get("companyName", "N/A")) # Fallback to companyName

    advertiser_data = job.get("advertiser", {})
    advertiser = advertiser_data.get("description", "N/A") if advertiser_data else "N/A"

    jobsdb_id = job.get("id", "N/A")
    locations_data = job.get("locationHierarchy", {}) # Use locationHierarchy
    # Combine location levels if they exist
    locations = ", ".join(filter(None, [
        locations_data.get("country", {}).get("name"),
        locations_data.get("state", {}).get("name"),
        locations_data.get("city", {}).get("name"),
        locations_data.get("area", {}).get("name")
    ])) or "N/A"

    # Handle potentially complex salary structures
    salary_data = job.get("salary", {})
    if isinstance(salary_data, dict):
        salary_text = salary_data.get("label", {}).get("text") # Prefer label text
        if not salary_text:
             # Fallback to constructing from min/max/type if label missing
             min_sal = salary_data.get("min")
             max_sal = salary_data.get("max")
             sal_type = salary_data.get("type")
             if min_sal and max_sal and sal_type:
                 salary_text = f"{min_sal:,} - {max_sal:,} ({sal_type})"
             elif min_sal and sal_type:
                  salary_text = f"From {min_sal:,} ({sal_type})"
             elif sal_type:
                  salary_text = f"Salary Type: {sal_type}"
             else:
                 salary_text = "Not Specified"
        salary = salary_text
    elif isinstance(salary_data, str): # Handle simple string salary
        salary = salary_data
    else:
        salary = "Not Specified"

    bullet_points = job.get("bulletPoints", [])
    teaser = job.get("teaser", "")
    classification = job.get("classification", {}).get("description", "N/A") # Job function/category


    formatted = (
        f"Platform       : JobsDB\n" # Add platform marker
        f"Job ID         : {jobsdb_id}\n"
        f"Title          : {title}\n"
        f"Company        : {company}\n"
        f"Advertiser     : {advertiser}\n"
        f"Location(s)    : {locations}\n"
        f"Salary         : {salary}\n"
        f"Classification : {classification}\n"
        f"Teaser         : {teaser}\n"
        f"Highlights     :\n"
    )
    if bullet_points:
        for bp in bullet_points:
            formatted += f"  - {bp}\n"
    else:
        formatted += "  N/A\n"
    # Add other potentially useful fields if needed (work type, etc.)
    return formatted

def scrape_common(url: str, return_html: bool = False) -> Optional[str]:
    """
    Common scraping logic with headers, delay, timeout, and error handling.
    Can return full HTML or just extracted text.
    """
    user_agents = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.3.1 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:124.0) Gecko/20100101 Firefox/124.0"
    ]
    headers = {
        'User-Agent': random.choice(user_agents),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Referer': 'https://www.google.com/', # Generic referer
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'DNT': '1',
        'Sec-GPC': '1',
         # Add TE header, sometimes helps
        'TE': 'trailers'
    }
    try:
        # Random delay to mimic human behavior
        delay = random.uniform(2.0, 5.0) # Increased delay range
        logger.debug(f"Scraping URL: {url} (Delay: {delay:.2f}s)")
        time.sleep(delay)

        response = requests.get(url, headers=headers, timeout=30) # Increased timeout
        response.raise_for_status() # Check for HTTP errors (4xx, 5xx)

        content_type = response.headers.get('Content-Type', '').lower()
        if 'html' not in content_type:
            logger.warning(f"Non-HTML content type '{content_type}' received from {url}")
            return f"Error: Received non-HTML content ({content_type}) from {url}"

        if return_html:
            logger.debug(f"Successfully scraped HTML from {url} (Length: {len(response.text)}).")
            return response.text # Return raw HTML
        else:
            # Parse HTML and extract text
            soup = BeautifulSoup(response.content, 'html.parser')

            # Remove common noise elements
            for element in soup(["script", "style", "header", "footer", "nav", "aside", "form", "button", "input", "meta", "link"]):
                element.decompose()

            # Get text with basic line separation, stripping extra whitespace
            text = soup.get_text(separator='\n', strip=True)
            # Further clean up multiple blank lines
            text = re.sub(r'\n\s*\n', '\n', text)

            if not text.strip():
                 logger.warning(f"Extracted text from {url} is empty after cleaning.")
                 # Check for common block indicators in raw HTML before returning error
                 raw_html_lower = response.text.lower()
                 if "captcha" in raw_html_lower or "authenticate" in raw_html_lower or "access denied" in raw_html_lower:
                     logger.error(f"Scraping likely blocked for {url} (Captcha/Auth/Block detected).")
                     return f"Error: Access denied or blocked while scraping {url}"
                 return f"Error: Extracted text from {url} is empty."


            logger.debug(f"Successfully scraped and extracted text from {url} (Length: {len(text)}).")
            return text

    except requests.exceptions.Timeout:
        logger.error(f"Timeout error fetching URL {url}")
        return f"Error: Timeout occurred while trying to fetch {url}"
    except requests.exceptions.HTTPError as http_err:
         logger.error(f"HTTP error {http_err.response.status_code} fetching URL {url}: {http_err}")
         # Check for common blocking status codes
         status = http_err.response.status_code
         if status in [403, 429, 503]: # Forbidden, Too Many Requests, Service Unavailable
              logger.warning(f"Potential blocking detected for {url} (Status: {status}).")
              return f"Error: Failed to fetch {url} - Access potentially blocked (Status: {status})"
         return f"Error: Failed to fetch {url} - HTTP status {status}: {http_err}"
    except requests.exceptions.RequestException as e:
        logger.error(f"Request error fetching URL {url}: {e}")
        return f"Error: Failed to fetch {url} due to network or connection issue: {e}"
    except Exception as e:
        logger.error(f"Unexpected error scraping URL {url}: {e}", exc_info=True)
        return f"Error: An unexpected error occurred during scraping of {url}: {e}"

def scrape_all_text(url: str) -> Optional[str]:
    """Scrape and return cleaned text content from the given URL."""
    return scrape_common(url, return_html=False)

def scrape_full_html(url: str) -> Optional[str]:
    """Scrape and return the full raw HTML from the given URL."""
    return scrape_common(url, return_html=True)

def read_cv(cv_file_path: str) -> Optional[str]:
    """Read and return the content of a .docx file (CV)."""
    if not os.path.exists(cv_file_path):
        logger.error(f"CV file not found at {cv_file_path}")
        return None # Indicate file not found
    if not cv_file_path.lower().endswith(".docx"):
       logger.error(f"Invalid file type. Only .docx files are supported. Path: {cv_file_path}")
       return None # Indicate wrong file type
    try:
        logger.info(f"Reading CV file from {cv_file_path}")
        loader = Docx2txtLoader(cv_file_path)
        docs = loader.load()
        if not docs:
            logger.warning(f"No content extracted from CV file: {cv_file_path}")
            return "" # Return empty string if no content

        content = "\n".join(d.page_content for d in docs if hasattr(d, 'page_content'))
        logger.info(f"Successfully read CV file (Length: {len(content)}).")
        return content
    except Exception as e:
        logger.error(f"Error reading CV file {cv_file_path}: {e}", exc_info=True)
        return None # Indicate reading error

# ---------------- Prompt Template ----------------
# Updated prompt to incorporate LinkedIn, better search strategy, company focus
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """You are an expert Recruitment Assistant specializing in the Hong Kong job market (but adaptable to other locations if specified).
Your goal is to find the *single best* job match for the user based on their provided CV and any stated preferences.

Follow these steps methodically:

1.  **Analyze CV:** Thoroughly review the user's CV. Identify:
    * Key skills (technical, soft, domain-specific).
    * Years of relevant experience and seniority level.
    * Primary job functions and roles held.
    * Industries worked in.
    * Educational background.
    * Any explicitly stated career goals, location preferences, or salary expectations.

2.  **Develop Initial Search Strategy:** Based on the CV analysis, determine the most promising initial search approach:
    * Identify 1-2 core job titles or functions (e.g., "Software Engineer", "Project Manager", "Data Analyst").
    * List relevant keywords (e.g., "Python", "React", "AWS", "Agile", "Financial Reporting", "Stakeholder Management").
    * Consider the target location (default to Hong Kong unless specified otherwise).
    * Decide whether to start with JobsDB (`jobsdb_search`) or LinkedIn (`linkedin_search`). JobsDB is often good for local HK roles, LinkedIn for tech, multinational, or senior roles.
    * Consider if adding company names (e.g., "Google", "HSBC") or types (e.g., "big tech", "fintech startup", "investment bank") to the keywords is relevant based on the CV or user request.

3.  **Execute Initial Search:** Use the chosen tool (`jobsdb_search` or `linkedin_search`) with the identified keywords, location, and `page=1`.

4.  **Evaluate Initial Results:** Examine the job summaries returned.
    * Are they relevant to the user's profile (title, seniority, industry)?
    * Do any descriptions look promising?
    * Are there enough results, or too few/too many?
    * Note down the Job IDs and Platforms of 1-3 *most promising* candidates. If using LinkedIn, be aware results might be less reliable.

5.  **Refine Search Strategy (Iterative Process - CRITICAL):**
    * **If initial results are poor or insufficient:** *Think step-by-step* about why and how to improve. DO NOT give up after one search.
    * **Consider:**
        * **Adjusting Keywords:** Broader terms? More specific skills? Synonyms? Different job titles? Adding company names/types?
        * **Switching Platforms:** If JobsDB was poor, try `linkedin_search`. If LinkedIn failed or was poor, try `jobsdb_search`.
        * **Trying More Pages:** If results look okay but limited, try `page=2` or `page=3` on the *same platform* with the *same keywords*.
        * **Revising Location:** Confirm the location is correct.
    * **Explain your reasoning:** Briefly state *why* you are changing the search (e.g., "Initial JobsDB search for 'Software Engineer' was too broad, refining with keywords 'Backend Engineer Java AWS' on JobsDB page 1.", or "JobsDB yielded few relevant roles, trying LinkedIn with keywords 'Senior Project Manager PMP' in Hong Kong.").
    * **Execute Refined Search:** Use the appropriate tool with the new parameters.
    * **Evaluate Again:** Assess the new results. Repeat refinement if necessary (up to 2-3 refinement cycles).
    * **If multiple attempts fail:** If after 2-3 strategic search attempts across platforms/keywords you still have no promising candidates, clearly state this.

6.  **Extract Details:** Once you have 1-3 promising Job IDs (from *any* successful search), use the `extract_job_details` tool *one by one* for each ID, ensuring you specify the correct `platform` ('jobsdb' or 'linkedin'). Be prepared for extraction failures, especially on LinkedIn. If extraction fails for a specific job, note it and potentially rely on the search summary or discard it if the summary was too vague.

7.  **Compare and Analyze:** Critically compare the extracted details (title, summary) of the viable candidate jobs against the user's CV.
    * Evaluate the match for skills, experience level, responsibilities, and industry.
    * Identify strengths of the match and potential gaps.
    * Consider salary alignment if information is available.

8.  **Final Recommendation:**
    * Select the *single job* that represents the *best overall match* based on your analysis.
    * Provide: Job ID, Platform, Full Job Title, Company (if available in summary/details), and a clear explanation (2-4 sentences) of *why* it's the best fit, referencing specific CV points and job requirements.
    * If, after diligent searching and analysis, no suitable job is found, clearly state this and briefly explain why (e.g., "No roles matching the required seniority in 'Quantum Computing' were found on JobsDB or LinkedIn in Hong Kong," or "Found roles, but detail extraction failed, preventing full comparison.").

You MUST use the available tools to search for and analyze jobs. Do not invent job details. If scraping or extraction fails for a tool, report the error and adjust your strategy accordingly. Prioritize finding *one high-quality match* over many mediocre ones.""",
        ),
        MessagesPlaceholder(variable_name="chat_history", optional=True),
        ("human", "{input}"), # User query + CV content
        MessagesPlaceholder(variable_name="agent_scratchpad"), # Agent's work
    ]
)


# ---------------- Agent and Executor Initialization ----------------
# Define the tools the agent can use
tools = [jobsdb_search, linkedin_search, extract_job_details] # Add linkedin_search

# Create the agent
# Ensure the LLM supports tool calling (OpenAI models generally do)
agent = create_openai_tools_agent(llm, tools, prompt)

# Create the AgentExecutor
agent_executor = AgentExecutor(
    agent=agent,
    tools=tools,
    verbose=True, # Set True to see thought process, False for cleaner output
    handle_parsing_errors="Check your output and make sure it conforms!", # Provide specific guidance on parsing errors
    max_iterations=15, # Allow more steps for refinement
    # Early stopping can be added if needed, e.g., based on finding a good match
    # return_intermediate_steps=True # Set True to see tool calls/outputs separately
)
logger.info("Agent and Executor initialized with JobsDB and LinkedIn tools.")

# ---------------- Running the Agent ----------------
if __name__ == "__main__":
    cv_path = "TestCV.docx"  # <<< IMPORTANT: Update with your actual CV file path >>>
    # Example alternative: Use environment variable or command-line argument
    # cv_path = os.getenv("CV_PATH", "TestCV.docx")

    logger.info(f"Attempting to load CV from: {cv_path}")
    cv_content = read_cv(cv_file_path=cv_path)

    if cv_content is None:
        logger.critical("Failed to read CV content. Please check the file path and format (.docx). Exiting.")
        exit(1)
    elif not cv_content.strip():
        logger.warning(f"CV file '{cv_path}' was read but appears to be empty or contains only whitespace.")
        # Depending on use case, you might allow running without a CV, but this agent relies heavily on it.
        logger.critical("CV content is empty. Agent requires CV details to function effectively. Exiting.")
        exit(1)

    # Structure the input clearly for the agent
    initial_query_with_cv = f"""
User Request: Please find the single best job match for me in Hong Kong based on my CV. Analyze my skills and experience carefully. Consider roles from both JobsDB and LinkedIn. If initial searches aren't great, try refining keywords or switching platforms. Focus on roles in established tech companies or high-growth startups if relevant based on my profile.

My CV Content:
--- START CV ---
{cv_content}
--- END CV ---
"""

    logger.info("Invoking the agent executor...")
    start_time = time.time()
    try:
        # Input must be a dictionary with keys matching the prompt variables
        response = agent_executor.invoke({"input": initial_query_with_cv})
        end_time = time.time()
        logger.info(f"Agent execution finished in {end_time - start_time:.2f} seconds.")

        # The final answer from the agent is typically in the 'output' key
        final_answer = response.get("output", "Agent did not produce a final output.")

        print("\n" + "=" * 30 + " Agent's Final Recommendation " + "=" * 30)
        print(final_answer)
        print("=" * (60 + len(" Agent's Final Recommendation ")))

    except Exception as e:
        end_time = time.time()
        # Log the full exception traceback for debugging
        logger.critical(f"An error occurred during agent execution after {end_time - start_time:.2f} seconds: {e}", exc_info=True)
        print(f"\nAn critical error occurred during agent execution: {e}")
        print("Please check the logs for more details.")